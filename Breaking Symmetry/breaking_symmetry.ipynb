{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    global a, b, c\n",
    "    a = torch.nn.Linear(1,10)\n",
    "    b = torch.nn.Linear(10,10)\n",
    "    c = torch.nn.Linear(10,1)\n",
    "    \n",
    "x = torch.tensor([1.])\n",
    "\n",
    "def forward(x):\n",
    "    return c(b(a(x).relu()).relu())\n",
    "\n",
    "def backprop(y):\n",
    "    y.backward()\n",
    "    for layer in [c,b,a]:\n",
    "        for param in layer.parameters():\n",
    "            param.data -= .01 * param.grad\n",
    "            param.grad *= 0\n",
    "            \n",
    "def train():\n",
    "    y = forward(x)\n",
    "    backprop(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random weights and biases\n",
    "## train() allows us to shrink the output of neuron C, which has 10 inputs/fan-in-weights\n",
    "### no need to show that weights are asymmetrical, this is the usual case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3623], grad_fn=<ThAddBackward>)\n",
      "tensor([-0.3764], grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "initialize()\n",
    "print(train())\n",
    "print(train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random biases, No randomness in weights\n",
    "## we can break symmetry in weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize()\n",
    "for layer in [a,b,c]:\n",
    "    layer.weight.data = torch.ones_like(layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(c.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.9021, 0.8972, 0.9009, 0.9020, 0.8984, 0.8989, 0.9003, 0.9026, 0.8991,\n",
      "         0.8999]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([99.6101], grad_fn=<ThAddBackward>), None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(), print(c.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.8254, 0.8157, 0.8230, 0.8253, 0.8181, 0.8192, 0.8219, 0.8266, 0.8196,\n",
      "         0.8210]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([70.4516], grad_fn=<ThAddBackward>), None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(), print(c.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.7640, 0.7494, 0.7604, 0.7638, 0.7530, 0.7546, 0.7587, 0.7657, 0.7553,\n",
      "         0.7574]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([51.7601], grad_fn=<ThAddBackward>), None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(), print(c.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fan-ins are almost identical across neurons, elements within one neuron's fan-in are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.9588, 0.9966, 0.9575, 0.9558, 0.9942, 0.9811, 0.9962, 0.9648, 0.9706,\n",
       "         0.9973],\n",
       "        [0.9590, 0.9966, 0.9577, 0.9560, 0.9942, 0.9811, 0.9962, 0.9650, 0.9707,\n",
       "         0.9973],\n",
       "        [0.9589, 0.9966, 0.9576, 0.9558, 0.9942, 0.9811, 0.9962, 0.9649, 0.9706,\n",
       "         0.9973],\n",
       "        [0.9588, 0.9966, 0.9575, 0.9558, 0.9942, 0.9811, 0.9962, 0.9648, 0.9706,\n",
       "         0.9973],\n",
       "        [0.9590, 0.9966, 0.9577, 0.9559, 0.9942, 0.9811, 0.9962, 0.9650, 0.9707,\n",
       "         0.9973],\n",
       "        [0.9589, 0.9966, 0.9577, 0.9559, 0.9942, 0.9811, 0.9962, 0.9650, 0.9707,\n",
       "         0.9973],\n",
       "        [0.9589, 0.9966, 0.9576, 0.9558, 0.9942, 0.9811, 0.9962, 0.9649, 0.9707,\n",
       "         0.9973],\n",
       "        [0.9588, 0.9966, 0.9575, 0.9557, 0.9942, 0.9810, 0.9962, 0.9648, 0.9706,\n",
       "         0.9973],\n",
       "        [0.9589, 0.9966, 0.9577, 0.9559, 0.9942, 0.9811, 0.9962, 0.9649, 0.9707,\n",
       "         0.9973],\n",
       "        [0.9589, 0.9966, 0.9576, 0.9559, 0.9942, 0.9811, 0.9962, 0.9649, 0.9707,\n",
       "         0.9973]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.weight #there's a small amount of symmetry breaking, but the fan-ins are pretty similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after 25 epochs: fan-ins are different across neurons, elements within one neuron's fan-in are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.9193, 0.9966, 0.9151, 0.9091, 0.9942, 0.9768, 0.9962, 0.9387, 0.9545,\n",
       "         0.9973],\n",
       "        [0.9212, 0.9966, 0.9168, 0.9102, 0.9942, 0.9770, 0.9962, 0.9399, 0.9552,\n",
       "         0.9973],\n",
       "        [0.9197, 0.9966, 0.9152, 0.9090, 0.9942, 0.9769, 0.9962, 0.9390, 0.9547,\n",
       "         0.9973],\n",
       "        [0.9193, 0.9966, 0.9151, 0.9091, 0.9942, 0.9768, 0.9962, 0.9387, 0.9546,\n",
       "         0.9973],\n",
       "        [0.9207, 0.9966, 0.9163, 0.9096, 0.9942, 0.9770, 0.9962, 0.9396, 0.9550,\n",
       "         0.9973],\n",
       "        [0.9205, 0.9966, 0.9161, 0.9094, 0.9942, 0.9770, 0.9962, 0.9394, 0.9550,\n",
       "         0.9973],\n",
       "        [0.9199, 0.9966, 0.9155, 0.9092, 0.9942, 0.9769, 0.9962, 0.9391, 0.9548,\n",
       "         0.9973],\n",
       "        [0.9191, 0.9966, 0.9148, 0.9088, 0.9942, 0.9768, 0.9962, 0.9386, 0.9545,\n",
       "         0.9973],\n",
       "        [0.9204, 0.9966, 0.9160, 0.9094, 0.9942, 0.9769, 0.9962, 0.9394, 0.9549,\n",
       "         0.9973],\n",
       "        [0.9201, 0.9966, 0.9157, 0.9095, 0.9942, 0.9769, 0.9962, 0.9392, 0.9548,\n",
       "         0.9973]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(25):\n",
    "    train()\n",
    "b.weight #now the fan-ins are pretty different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No randomness in bias/weights\n",
    "## we can't break symmetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize()\n",
    "for layer in [a,b,c]:\n",
    "    layer.weight.data = torch.ones_like(layer.weight)\n",
    "    layer.bias.data = torch.ones_like(layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], requires_grad=True) Parameter containing:\n",
      "tensor([1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(c.weight, c.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.7900, 0.7900, 0.7900, 0.7900, 0.7900, 0.7900, 0.7900, 0.7900, 0.7900,\n",
      "         0.7900]], requires_grad=True) Parameter containing:\n",
      "tensor([0.9900], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([211.], grad_fn=<ThAddBackward>), None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(), print(c.weight, c.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.6037, 0.6037, 0.6037, 0.6037, 0.6037, 0.6037, 0.6037, 0.6037, 0.6037,\n",
      "         0.6037]], requires_grad=True) Parameter containing:\n",
      "tensor([0.9800], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([148.1670], grad_fn=<ThAddBackward>), None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(), print(c.weight, c.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.4350, 0.4350, 0.4350, 0.4350, 0.4350, 0.4350, 0.4350, 0.4350, 0.4350,\n",
      "         0.4350]], requires_grad=True) Parameter containing:\n",
      "tensor([0.9700], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([102.8286], grad_fn=<ThAddBackward>), None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(), print(c.weight, c.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fan-ins are identical across neurons, elements within one neuron's fan-in are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558,\n",
       "         0.9558],\n",
       "        [0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558,\n",
       "         0.9558],\n",
       "        [0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558,\n",
       "         0.9558],\n",
       "        [0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558,\n",
       "         0.9558],\n",
       "        [0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558,\n",
       "         0.9558],\n",
       "        [0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558,\n",
       "         0.9558],\n",
       "        [0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558,\n",
       "         0.9558],\n",
       "        [0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558,\n",
       "         0.9558],\n",
       "        [0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558,\n",
       "         0.9558],\n",
       "        [0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558, 0.9558,\n",
       "         0.9558]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after 25 epochs: fan-ins are identical across neurons, elements within one neuron's fan-in are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077,\n",
       "         25.1077, 25.1077],\n",
       "        [25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077,\n",
       "         25.1077, 25.1077],\n",
       "        [25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077,\n",
       "         25.1077, 25.1077],\n",
       "        [25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077,\n",
       "         25.1077, 25.1077],\n",
       "        [25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077,\n",
       "         25.1077, 25.1077],\n",
       "        [25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077,\n",
       "         25.1077, 25.1077],\n",
       "        [25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077,\n",
       "         25.1077, 25.1077],\n",
       "        [25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077,\n",
       "         25.1077, 25.1077],\n",
       "        [25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077,\n",
       "         25.1077, 25.1077],\n",
       "        [25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077, 25.1077,\n",
       "         25.1077, 25.1077]], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(25):\n",
    "    train()\n",
    "b.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
