{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong-Playing TensorFlow Neural Network\n",
    "\n",
    "## Import modules needed to train neural network in Pong environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the environment and set up data preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "env.render(mode='rgb_array').shape\n",
    "env.reset()\n",
    "print(env.action_space)\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "\n",
    "top = 32\n",
    "bottom = 195\n",
    "left = 14\n",
    "right = 146\n",
    "downsampled_height = int(np.rint((bottom-top)/2))\n",
    "downsampled_width = int(np.rint((right-left)/2))\n",
    "input_dim = downsampled_height*downsampled_width\n",
    "\n",
    "def preprocess(img, reshape=False):\n",
    "    #crop, grab only one channel, and downsample by factor of 2\n",
    "    img = img[top:bottom,left:right,0][::2,::2] \n",
    "    #get rid of background color, 109 in first frame, 144 otherwise\n",
    "    img[np.isin(img,[144,109])] = 0 \n",
    "    img[img!=0] = 1\n",
    "    if not reshape:\n",
    "        return img.astype(np.int).ravel()\n",
    "    else:\n",
    "        return img.astype(np.int)\n",
    "\n",
    "def reshape(img):\n",
    "    return img.reshape(downsampled_height,downsampled_width).astype(np.int)\n",
    "\n",
    "#what color pixels are in this image?\n",
    "#print(list(zip(*np.unique(env.render(mode='rgb_array')[top:bottom,left:right,0],return_counts=1))))\n",
    "#print(list(zip(*np.unique(env.render(mode='rgb_array')[top:bottom,left:right,0][::2,::2],return_counts=1))))\n",
    "\n",
    "plt.subplots(2,3, figsize=(12,10))\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"The Atari Pong Game Screen\")\n",
    "plt.imshow(env.reset())\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"Cropped, First Channel Only\")\n",
    "plt.imshow(env.render(mode='rgb_array')[top:bottom,left:right,0])\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.title(\"Prior Plus Downsample\")\n",
    "plt.imshow(env.render(mode='rgb_array')[top:bottom,left:right,0][::2,::2])\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(\"After a step, color scheme changes\")\n",
    "plt.imshow(env.step(2)[0][top:bottom,left:right,0][::2,::2])\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(\"After Preprocessing Frame 1\")\n",
    "plt.imshow(reshape(preprocess(env.reset())))\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(\"After Preprocessing Frame 2\")\n",
    "plt.imshow(reshape(preprocess(env.step(2)[0])))\n",
    "plt.show()\n",
    "\n",
    "#print(list(zip(*np.unique(env.render(mode='rgb_array')[top:bottom,left:right,0][::2,::2],return_counts=1))))\n",
    "#print(list(zip(*np.unique(reshape(preprocess(env.reset())),return_counts=1))))\n",
    "#print(list(zip(*np.unique(reshape(preprocess(env.render(mode='rgb_array'))),return_counts=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and initialize the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, input_dim])\n",
    "advantage = tf.placeholder(tf.float32, shape=[None])\n",
    "action = tf.placeholder(tf.int8, shape=[None])\n",
    "move_down_action = tf.Variable(3, dtype=tf.int8)\n",
    "\n",
    "h1_dim = 200\n",
    "l1 = tf.layers.dense(x, h1_dim, activation=tf.nn.relu)\n",
    "move_down_prob = tf.layers.dense(l1, 1, activation=tf.nn.sigmoid)\n",
    "sampled_action_log_prob = tf.where(tf.equal(action, move_down_action),\n",
    "                                     tf.log(move_down_prob),\n",
    "                                     tf.log(1-move_down_prob))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "train = optimizer.minimize(tf.reduce_sum(-advantage*sampled_action_log_prob))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up an agent class that plays pong using actions chosen by the neural network in the active TensorFlow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class pong_agent:\n",
    "        \n",
    "    def clean_slate(self):    \n",
    "        self.wins = 0\n",
    "        self.games = 0    \n",
    "        self.p_list = []\n",
    "        self.actions = []\n",
    "        self.frames = []\n",
    "        self.frame_changes = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def make_batch(self, n_sets):\n",
    "        self.clean_slate()\n",
    "        for _ in range(n_sets):\n",
    "            self.play_set()\n",
    "        self.normalize_rewards()\n",
    "        return self.rewards, self.frame_changes, self.actions\n",
    "\n",
    "    def play_set(self):\n",
    "        env.reset()\n",
    "        done = 0\n",
    "        self.frames.append(preprocess(env.render(mode='rgb_array')))\n",
    "        self.frame_changes.append(self.frames[-1] - self.frames[-1])\n",
    "        while not done:\n",
    "            done = self.play_point()\n",
    "\n",
    "    def play_point(self):\n",
    "        frames_played = 0\n",
    "        discount = 0.99\n",
    "        while True:\n",
    "            prob, action, reward, new_frame, done = self.play_frame(self.frame_changes[-1])\n",
    "            self.p_list.append(prob)\n",
    "            self.actions.append(action)\n",
    "            frames_played+= 1\n",
    "            if reward:\n",
    "                self.rewards+= [reward * discount**k for k in reversed(range(frames_played))]\n",
    "                self.wins+= max(reward,0)\n",
    "                self.games+= 1\n",
    "                if not done:\n",
    "                    self.frames.append(new_frame)\n",
    "                    self.frame_changes.append(self.frames[-1] - self.frames[-2])\n",
    "                break\n",
    "        return done\n",
    "\n",
    "    def play_frame(self, frame_change):\n",
    "        p_down = sess.run(move_down_prob, feed_dict={x:np.array([frame_change])}).reshape(1)[0]\n",
    "        #sample an action using p_down, 3=down, 2=up\n",
    "        action = np.random.binomial(1, p_down) + 2 \n",
    "        observation, reward, done = env.step(action)[:3]\n",
    "        return p_down, action, reward, preprocess(observation), done\n",
    "\n",
    "    def normalize_rewards(self):\n",
    "        mean = np.mean(self.rewards)\n",
    "        std_dev = np.std(self.rewards)\n",
    "        self.rewards = (np.array(self.rewards)-mean)/std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent for 2000 updates to reach >50% win rate\n",
    "\n",
    "This part takes a while. My setup processes ~100 batches/hour. To monitor progress (the agent's win rate), I output at a .PNG plot every 10 batches. There's a visible shift in the win rate from ~2% to ~4% by batch 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ratios = []\n",
    "matches_per_batch = 10\n",
    "epochs = 2000\n",
    "agent = pong_agent()\n",
    "\n",
    "for i in range(epochs):\n",
    "    #play Pong with the network, save frames and associated rewards\n",
    "    rewards, frame_changes, actions = agent.make_batch(matches_per_batch)\n",
    "    train.run(feed_dict={x: frame_changes, advantage: rewards, action: actions})\n",
    "    ratios.append(agent.wins/agent.games*100)\n",
    "    if i%10==0:\n",
    "        print(\"{}: batch {} finished after {} hours\".format(time.strftime('%X %x '), \n",
    "                                                    i, round((time.time()-start)/3600,2)))\n",
    "        plt.title(\"Agent Quality over Time\")\n",
    "        plt.plot(range(1,i+2), ratios)\n",
    "        plt.xlabel(\"Number of Updates\")\n",
    "        plt.ylabel(\"Percent of Games Won\")\n",
    "        plt.savefig(\"./pong_agent_quality\")\n",
    "        if i%100==0:\n",
    "            #save out the neural network's weights here\n",
    "            saver.save(sess, \"./pong_agent.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the last game played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "watch the agent's Pong play in the most recent game. the right side of the plot is the \n",
    "difference frame that the agent/neural-network uses to predict a good action to take\n",
    "'''\n",
    "def concat(one, two):\n",
    "    border = np.ones((downsampled_height,3))*3\n",
    "    return np.concatenate([one,border,two], axis=1)\n",
    "\n",
    "duration = 500 # number of frames to watch\n",
    "img = plt.imshow(concat(reshape(agent.frames[0]) , reshape(frame_changes[1])))\n",
    "for i in range(min(len(agent.frames), duration)):\n",
    "    img.set_data(concat(reshape(agent.frames[i]) , reshape(frame_changes[i])))\n",
    "    plt.title(\"Move Down with Probability %.2f\" % agent.p_list[i])\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
